
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Datasets &#8212; Deep_Learning_NLP 0.0.1 documentation</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.0.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Contributing" href="../credentials/CONTRIBUTING.html" />
    <link rel="prev" title="Tutorials" href="tutorials.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="datasets">
<h1>Datasets<a class="headerlink" href="#datasets" title="Permalink to this headline">¶</a></h1>
<div class="section" id="general">
<h2>General<a class="headerlink" href="#general" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><strong>1 Billion Word Language Model Benchmark</strong>: The purpose of the project is to make available a standard training and test setup for language modeling experiments:
[<a class="reference external" href="http://www.statmt.org/lm-benchmark/">Link</a>]</li>
<li><strong>Common Crawl</strong>: The Common Crawl corpus contains petabytes of data collected over the last 7 years. It contains raw web page data, extracted metadata and text extractions:
[<a class="reference external" href="http://commoncrawl.org/the-data/get-started/">Link</a>]</li>
<li><strong>Yelp Open Dataset</strong>: A subset of Yelp’s businesses, reviews, and user data for use in personal, educational, and academic purposes:
[<a class="reference external" href="https://www.yelp.com/dataset">Link</a>]</li>
</ul>
</div>
<div class="section" id="text-classification">
<h2>Text classification<a class="headerlink" href="#text-classification" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><strong>20 newsgroups</strong> The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups:
[<a class="reference external" href="http://qwone.com/~jason/20Newsgroups/">Link</a>]</li>
<li><strong>Broadcast News</strong> The 1996 Broadcast News Speech Corpus contains a total of 104 hours of broadcasts from ABC, CNN and CSPAN television networks and NPR and PRI radio networks with corresponding transcripts:
[<a class="reference external" href="https://catalog.ldc.upenn.edu/LDC97S44">Link</a>]</li>
<li><strong>The wikitext long term dependency language modeling dataset</strong>: A collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. :
[<a class="reference external" href="https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset">Link</a>]</li>
</ul>
</div>
<div class="section" id="question-answering">
<h2>Question Answering<a class="headerlink" href="#question-answering" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><strong>Question Answering Corpus</strong> by Deep Mind and Oxford which is two new corpora of roughly a million news stories with associated queries from the CNN and Daily Mail websites.
[<a class="reference external" href="https://github.com/deepmind/rc-data">Link</a>]</li>
<li><strong>Stanford Question Answering Dataset (SQuAD)</strong> consisting of questions posed by crowdworkers on a set of Wikipedia articles:
[<a class="reference external" href="https://rajpurkar.github.io/SQuAD-explorer/">Link</a>]</li>
<li><strong>Amazon question/answer data</strong> contains Question and Answer data from Amazon, totaling around 1.4 million answered questions:
[<a class="reference external" href="http://jmcauley.ucsd.edu/data/amazon/qa/">Link</a>]</li>
</ul>
</div>
<div class="section" id="sentiment-analysis">
<h2>Sentiment Analysis<a class="headerlink" href="#sentiment-analysis" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><strong>Multi-Domain Sentiment Dataset</strong> TThe Multi-Domain Sentiment Dataset contains product reviews taken from Amazon.com from many product types (domains):
[<a class="reference external" href="http://www.cs.jhu.edu/~mdredze/datasets/sentiment/">Link</a>]</li>
<li><strong>Stanford Sentiment Treebank Dataset</strong> The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language:
[<a class="reference external" href="https://nlp.stanford.edu/sentiment/">Link</a>]</li>
<li><strong>Large Movie Review Dataset</strong>: This is a dataset for binary sentiment classification:
[<a class="reference external" href="http://ai.stanford.edu/~amaas/data/sentiment/">Link</a>]</li>
</ul>
</div>
<div class="section" id="machine-translation">
<h2>Machine Translation<a class="headerlink" href="#machine-translation" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><strong>Aligned Hansards of the 36th Parliament of Canada</strong> dataset contains 1.3 million pairs of aligned text chunks:
[<a class="reference external" href="https://www.isi.edu/natural-language/download/hansard/">Link</a>]</li>
<li><strong>Europarl: A Parallel Corpus for Statistical Machine Translation</strong> dataset extracted from the proceedings of the European Parliament:
[<a class="reference external" href="http://www.statmt.org/europarl/">Link</a>]</li>
</ul>
</div>
<div class="section" id="summarization">
<h2>Summarization<a class="headerlink" href="#summarization" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><strong>Legal Case Reports Data Set</strong> as a textual corpus of 4000 legal cases for automatic summarization and citation analysis.:
[<a class="reference external" href="https://archive.ics.uci.edu/ml/datasets/Legal+Case+Reports">Link</a>]</li>
</ul>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="tutorials.html" title="previous chapter">Tutorials</a></li>
      <li>Next: <a href="../credentials/CONTRIBUTING.html" title="next chapter">Contributing</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2018, Amirsina Torfi.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.6</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
      |
      <a href="../_sources/content/datasets.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>